#!/usr/bin/env python3
"""
MLX-based Reinforcement Learning Training Script
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path

try:
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
except ImportError:
    print("MLX not found. This script requires Apple Silicon and MLX.")
    print("Install with: pip install mlx")
    exit(1)

import numpy as np
from tqdm import tqdm


class SimpleAgent:
    """Simple MLX-based agent for demonstration"""
    
    def __init__(self, state_dim=4, action_dim=2, hidden_dim=64):
        self.model = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        self.optimizer = optim.Adam(learning_rate=0.001)
        
    def forward(self, x):
        return self.model(x)
    
    def train_step(self, states, actions, rewards):
        """Simple training step - replace with your RL algorithm"""
        def loss_fn():
            predictions = self.model(states)
            # Simplified loss for demonstration
            return mx.mean((predictions - rewards) ** 2)
        
        loss, grads = mx.value_and_grad(loss_fn)(self.model.parameters())()
        self.optimizer.update(self.model, grads)
        return loss.item()


def train_agent(episodes=100, output_dir="docs/results"):
    """Main training loop"""
    print(f"Starting MLX RL training for {episodes} episodes...")
    
    agent = SimpleAgent()
    
    # Training metrics
    episode_rewards = []
    episode_lengths = []
    losses = []
    
    for episode in tqdm(range(episodes), desc="Training"):
        # Simulate environment interaction
        # Replace this with actual environment like Gymnasium
        episode_reward = 0
        episode_length = 0
        
        for step in range(100):  # Max 100 steps per episode
            # Simulate state, action, reward
            state = mx.random.normal([4])  # 4D state
            action = mx.random.normal([2])  # 2D action
            reward = mx.random.normal([1]) * 0.1 + mx.sin(step * 0.1)
            
            # Simple training step
            loss = agent.train_step(
                mx.expand_dims(state, 0),
                mx.expand_dims(action, 0), 
                mx.expand_dims(reward, 0)
            )
            
            episode_reward += reward.item()
            episode_length += 1
            
            # Simple termination condition
            if reward.item() > 0.5 or step >= 99:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        if episode % 10 == 0:
            losses.append(loss)
    
    # Save results
    results = {
        'episodes': list(range(len(episode_rewards))),
        'rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'losses': losses,
        'timestamp': datetime.now().isoformat(),
        'total_episodes': episodes,
        'framework': 'MLX',
        'agent_type': 'SimpleAgent'
    }
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    with open(f"{output_dir}/latest_run.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"Training completed!")
    print(f"Average reward: {np.mean(episode_rewards):.2f}")
    print(f"Max reward: {np.max(episode_rewards):.2f}")
    print(f"Results saved to {output_dir}/latest_run.json")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='Train MLX RL Agent')
    parser.add_argument('--episodes', type=int, default=100, 
                       help='Number of training episodes')
    parser.add_argument('--output', type=str, default='docs/results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    train_agent(episodes=args.episodes, output_dir=args.output)


if __name__ == "__main__":
    main()
